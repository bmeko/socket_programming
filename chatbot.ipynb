{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcEI+jRY0gr8woDshkTJMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bmeko/socket_programming/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "oi92A4CaGqwc"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow \n",
        "tensorflow.device('/device:GPU:0')\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "data = json.load(open(\"/content/chat-bot/intents.json\"))\n",
        "\n",
        "intents=data[\"data\"][0]\n",
        "catagory=data[\"data\"][1]\n",
        "\n",
        "words=[]\n",
        "conver=[]\n",
        "documents=[]\n",
        "ignore_letters = [\"?\", \"!\",\".\", \",\", \"is\", \"and\", \"of\"] \n",
        "\n",
        "for intent in intents[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "    word_list=nltk.word_tokenize(pattern)\n",
        "    words.extend(word_list)\n",
        "    documents.append((word_list,intent[\"tag\"]))\n",
        "    if intent[\"tag\"] not in conver:\n",
        "      conver.append(intent[\"tag\"])\n",
        "\n",
        "words=[lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "words=sorted(set(words))\n",
        "\n",
        "conver=sorted(set(conver))\n",
        "\n",
        "pickle.dump(words,open('/content/chat-bot/words.pkl','wb'))\n",
        "pickle.dump(conver,open('/content/chat-bot/conver.pkl','wb'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nJGwdkbJIAOa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit.completion import word_completer\n",
        "training=[]\n",
        "output_empty= [0]* len(conver)\n",
        "\n",
        "\n",
        "for document in documents:\n",
        "  bag=[]\n",
        "  word_patterns= document[0]\n",
        "  word_patterns= [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "  \n",
        "  output_row = list(output_empty)\n",
        "  output_row[conver.index(document[1])]=1\n",
        "  training.append([bag,output_row])\n",
        "  \n",
        "\n",
        "random.shuffle(training)\n",
        "training=np.array(training)\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),),activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "hist  = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=10, verbose=1)\n",
        "model.save('conversation_model.model', hist)\n",
        "print(\"done\")\n"
      ],
      "metadata": {
        "id": "GaVTgrumZKDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_t=pickle.load(open(\"/content/chat-bot/words.pkl\",'rb'))\n",
        "conver_t=pickle.load(open(\"/content/chat-bot/conver.pkl\",'rb'))\n",
        "model= load_model('conversation_model.model')\n",
        "def clean_sent(sent):\n",
        "  sent_words=nltk.word_tokenize(sent)\n",
        "  sent_words=[lemmatizer.lemmatize(word) for word in sent_words]\n",
        "  return sent_words\n",
        "\n",
        "def bag_of_word(sent):\n",
        "  sent_words= clean_sent(sent)\n",
        "  bag = [0]*len(words_t)\n",
        "  for w in sent_words:\n",
        "    for i, word in enumerate(words):\n",
        "      if word == w:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)\n",
        "\n",
        "def predict_conv(sent):\n",
        "  bow = bag_of_word(sent)\n",
        "  res= model.predict(np.array([bow]))[0]\n",
        "  error_thresh=0.25\n",
        "  results =[[i,r] for i, r  in enumerate(res) if r> error_thresh]  \n",
        "  \n",
        "  results.sort(key=lambda x:x[1], reverse=True)\n",
        "  return_list = []\n",
        "  print(results)\n",
        "  for r in results:\n",
        "    return_list.append({'intent': conver_t[r[0]],'probability': str(r[1])})\n",
        "  print(return_list)\n",
        "\n",
        "while True:\n",
        "  message = input(\">>\")\n",
        "  ints= predict_conv(message)"
      ],
      "metadata": {
        "id": "DwaW-5b6iQ7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}